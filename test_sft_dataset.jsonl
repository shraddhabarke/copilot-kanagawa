{"prompt": "Performs the encoding for input tokens into Huffman codes. Also responsible for maintaining the number of uncompressed bytes processed and draining the contents of _preparedBuffer containing the GZIP header and/or starting or transitioning onto a new DEFLATE block, as well as to output the GZIP footer.", "completion": "[[pipelined]] void encode_pipeline(index_t<MaxEncodePipelineThreads> tid, token_t[Width] word,\ncount_t<Width> wordSize, index_t<MaxEncodePipelineThreads> dataTid,\nbool flush)\n{\nuint<MaxWriteBits * MaxWriteThreads> data;\ncount_t<MaxWriteBits * MaxWriteThreads> size;\noptional<token_t>[Width] optionalWord;\nauto mask = mask_less_than<Width>(wordSize);\nstatic for(const auto i : Width)\noptionalWord[i] = make_optional(mask[i], word[i]);\n//Find the original/dereferenced size by accounting for reference lengths\nauto origSize = map_reduce(token_length<token_t, count_t<MaxReferenceDistance*Width>>\n, add<count_t<MaxReferenceDistance*Width>, count_t<MaxReferenceDistance*Width>>\n, optionalWord);\nbool endBlock = (wordSize == 0);\nbool padToNextByte = false;\nuint64 snappedBytesEncoded;\nbool snappedPrevEndBlock;\natomic\n{\nif (tid == dataTid)\n_bytesUncompressed += origSize;\nsnappedBytesEncoded = _bytesUncompressed;\nstatic bool prevEndBlock = false;\nsnappedPrevEndBlock = prevEndBlock;\nprevEndBlock = endBlock;\n}\nif (tid < dataTid)\n{\n// Write contents of _preparedBuffer, containing end-of-last-block (if appropriate) and\n// header-of-next-block\natomic\n{\nsim_assert(tid < _preparedBufferCount);\ndata = _preparedBuffer[tid];\nstatic assert(MaxPreparedBufferSize <= bitsizeof(data));\n}\nsize = (tid == dataTid - 1) ? _lastPreparedBufferEntrySize : MaxPreparedBufferSize;\n}\nelse if (tid == dataTid){\nuint1 snappedCodesFront;\natomic {\n// If this is not the first thread, then it must mean a new header was written so switch over\n// to those new data codes\nif (tid != 0)\n_codebookFront = ~_codebookFront;\nsnappedCodesFront = _codebookFront; }\n// Lack of any data implies an end-of-block operation\nif (endBlock) {\n// Write DEFLATE end-of-block (if one was not written previously and no header was just emitted)\nauto c = code_EOB(snappedCodesFront);\ndata = c.code;\nsize = (!snappedPrevEndBlock && tid == 0) ? c.length : 0;\n}\nelse {\n// Write data\ntokenCode_t<MaxEncodedBits>[Width] compressed;\nstatic for(const auto i : Width)\n{\nif (optionalWord[i].is_valid) {\nlengthSymbolWithExtraBits_t sb;\nif (word[i].kind == input_kind::reference)\nsb = length_to_symbol_and_extra_bits<lengthSymbolWithExtraBits_t,MaxReferenceLength>(\nword[i].payload.reference.length);\nauto s = (word[i].kind == input_kind::data ? word[i].payload.data : sb.sym);\nauto c = _litLenCodebook[snappedCodesFront * HLITPow2 | s];\nsim_assert(c.length != 0);\ncompressed[i] = {c.code, c.length};\nif (word[i].kind == input_kind::reference) {\ncompressed[i].code |= sb.bits << compressed[i].length;\ncompressed[i].length += sb.extraBits;\nauto db = distance_to_symbol_and_extra_bits<distanceSymbolWithExtraBits_t,MaxReferenceDistance>(\nword[i].payload.reference.offset);\nauto c = _distCodebook[snappedCodesFront * HDISTPow2 | db.sym];\nsim_assert(c.length != 0);\ncompressed[i].code |= ((db.bits << c.length) | c.code) << compressed[i].length;\ncompressed[i].length += db.extraBits + c.length;}}\nelse\nsim_assert(compressed[i].length == 0);\n}// Reduce\nauto result = reduce(reduce_token_code<MaxEncodedBits>, compressed);\ndata = result.code;\nsize = result.length;\nsim_assert(size != 0);\n}\n}\nelse if (tid == dataTid + 1) {\n// Write BFINAL block (using fixed Huffman)\nsim_assert(dataTid == 0);\nsim_assert(endBlock);\ndata = (1 << 0) |                   // BFINAL\n(gz::btype::fixed << 1) |    // BTYPE == 0b01\n(0 << 3);                    // EndOfBlockSymbol\nsize = 1 + 2 + 7;\n// Pad to next byte boundary\npadToNextByte = true; } else if (tid == dataTid + 2) {\n// Write GZIP footer: CRC32\nstatic assert(MaxEncodedBits >= 32);\ndata = cast<uint32>(_crc32);\nsize = 32;\n}\nelse {\nsim_assert(flush);\n// Write GZIP footer: ISIZE\nsim_assert(tid == dataTid + 3);\nstatic assert(MaxEncodedBits >= 32);\ndata = snappedBytesEncoded;\nsize = 32;\nstatic assert(32 / MaxWriteBits + 1 /* flush */ <= MaxWriteThreads);\n}\nflush &&= (tid == dataTid + 3);\natomic {\nstatic index_t<8> accumulatedBits = 0;\naccumulatedBits += size;\nif (padToNextByte && accumulatedBits != 0) {\nsize += 8 - accumulatedBits;\naccumulatedBits = 0;\n}\nif (flush) {\n// By flush time should already be byte aligned\nsim_assert(accumulatedBits == 0);\n} }\n// Note that the input FIFO to write_async could fill up and apply back-pressure upstream and limit\n// compressor throughput.\n// On one hand, this can occur when a sequence of input words (e.g. containing long literal/length &\n// distance codes, accompanied by extra bits) each consistently generates more than MaxWriteBits of data,\n// thus queueing up more than one thread into write_async() per cycle.\n// On the other hand, outside of synthetic testing, the use of lzcomp alongside this compressor would not\n// be expected to generate the full Width tokens per cycle (for example, since any back-references must\n// cover a minimum of three literals) for back-pressure to be a problem.\nauto numThreads = (size + MaxWriteBits - 1) / MaxWriteBits + (flush ? 1 : 0);\nwrite_async(numThreads, cast<uint<MaxWriteBits>[MaxWriteThreads]>(data), size, flush);\n}"}
{"prompt": "Verilog style NAND reduction operator", "completion": "\ntemplate<typename T>\ninline bool reduction_nand(T x) { return !reduction_and(x); }\n"}
{"prompt": "Fail test with the specified tag when the condition is false. Test failure triggers `sim_assert`, prints information about failing test, and sets global status to the test tag.", "completion": "inline void assert(tag_t tag, bool cond)\n{\n    if (!cond)\n    {\n        print_failure(tag);\n        sim_assert(false);\n    }\n}"}
{"prompt": "Check `in1` greater than `in2`.", "completion": "template <Denorm denorm_mode>\ninline bool gt(uint64 in1, uint64 in2)\n{\n    // Unpack input\n    float64PackedFormat a;\n    float64PackedFormat b;\n    a = cast<float64PackedFormat>(in1);\n    b = cast<float64PackedFormat>(in2);\n\n    // Get zero, nan, or inf\n    specialCaseFlags a_flags = getSpecialCases<denorm_mode>(a);\n    specialCaseFlags b_flags = getSpecialCases<denorm_mode>(b);\n\n    bool result;\n    if (a_flags.nan || b_flags.nan)\n    {\n        result = false;\n    }\n    else\n    {\n        result = !eq<denorm_mode>(in1, in2) && !lt<denorm_mode>(in1, in2);\n    }\n    return result;\n}"}
{"prompt": "A mechanism for converting values among differing types.", "completion": "template\n< typename From                                        //< Type of the value to be\n// converted.\n, typename To                                          //< Type of the result of the\n// conversion.\n, (To, bool, count_t<bytesizeof(To)>) -> void Callback //< The callback function to be\n// invoked when the conversion is\n// complete.\n>\nclass gearbox\n{\nprivate:\nconst auto output_array_length = bitsizeof(To) > bitsizeof(From) ? 1 : bitsizeof(From) / bitsizeof(To);\ngearbox_context<From, To> context;\n\n[[pipelined]] void flush_output(count_t<output_array_length> tid, gearbox_context<From, To> ctx)\n{\nbool last = false;\nauto bytes = bytesizeof(To);\n\nif (tid == ctx.num_output_elements - 1)\n{\nbytes -= (bytesizeof(To)) - ctx.num_bytes;\nlast = true;\n} Callback(ctx.u.output_buff[tid], last, bytes);}"}
{"prompt": "Return the maximum number of elements.", "completion": "template <typename T, auto N /*< Maximum size.*/>\nstruct vector\n{\n    T[N] data;\n    count_t<N> size;\n}\n\ntemplate <typename T, auto N>\ninline count_t<N> capacity(vector<T, N> v)\n{\n    return N;\n}\n"}
{"prompt": "Returns `(a + 1) mod M`. // `a` must be less than `M`.", "completion": "inline auto increment(A a)\n{\n    return add<A, uint1, M>(a, 1);\n}"}
{"prompt": "Reserve `M` number of elements. If `M` is less than `v.size` then a `sim_assert` is triggered. Reserving the same capacity as the input vector returns the orginal vector.", "completion": "template <typename T, auto N /*< Maximum size.*/>\nstruct vector\n{\n    T[N] data;\n    count_t<N> size;\n}\n\ntemplate <auto M, typename T, auto N>\ninline vector<T, M> reserve(vector<T, N> v)\n{\n    sim_assert(v.size <= M);\n    const auto Bound = M < N ? M : N;\n    vector<T, M> new;\n    new.size = v.size;\n    static for(const auto i : Bound)\n        new.data[i] = v.data[i];\n    return new;\n}"}
{"prompt": "Functions annotated with [[reset]] attribute are called automatically after the design is reset (typically used to initialize state). Reset functions must have void return type and an empty parameter list.", "completion": "class Foo{\nprivate:\n    [[reset]] void init()  {\n        pipelined_for(32, [](index_t<32> i) {\n            mem[i] = 0;\n        });\n    }\n    memory<uint32, 32> mem;\n}"}
{"prompt": "Structure of numbers representing $sin(x)$ and $cos(x)$. // Used for the FFT's twiddle factor table.", "completion": "template<typename NativeDataType> \nstruct sin_cos_t\n{\n    NativeDataType sin;\n    NativeDataType cos;\n}template<typename NativeDataType, auto MaxTwiddleFactors>\nclass processor\n{\nprivate:\n    using twiddle_addr_t = index_t<MaxTwiddleFactors>;\n    memory_norep<sin_cos_t<NativeDataType>, MaxTwiddleFactors> _twiddle_factors;\npublic:\n    inline sin_cos_t<NativeDataType> read_twiddle_factors(twiddle_addr_t read_address)\n    {\n        return (_twiddle_factors[read_address]);\n    }\n    inline void write_twiddle_factors(twiddle_addr_t write_address, sin_cos_t<NativeDataType> input_data)\n    {\n        _twiddle_factors[write_address] = input_data;\n    }}"}
{"prompt": "Check `in1` greater than `in2`.", "completion": "inline bool gt(float32 in1, float32 in2)\n{\n    // Unpack input\n    float32PackedFormat a;\n    float32PackedFormat b;\n    a = cast<float32PackedFormat>(in1);\n    b = cast<float32PackedFormat>(in2);\n\n    // Get zero, nan, or inf\n    specialCaseFlags a_flags = getSpecialCases<denorm_mode>(a);\n    specialCaseFlags b_flags = getSpecialCases<denorm_mode>(b);\n\n    bool result;\n    if (a_flags.nan || b_flags.nan)\n    {\n        result = false;\n    }\n    else\n    {\n        result = !eq<denorm_mode>(in1, in2) && !lt<denorm_mode>(in1, in2);\n    }\n    return result;\n}"}
{"prompt": "Write one entry to the FIFO. Block if FIFO is full. Return the number of dequeue calls that were released.", "completion": "count_t<Size> enqueue(T value, bool releaseDeferredDequeues)\n    {\n        // Block until the FIFO is no longer full\n        _writeSemaphore.wait();\n\n        // Get the value of the write pointer then increment the write pointer\n        auto producer_index = first(atomically<pointer_t>(increment));\n\n        // Store the value into the memory\n        _dataMem[producer_index] = value;\n\n        count_t<Size> snappedDeferredDequeueCount;\n        atomic\n        {\n            if (releaseDeferredDequeues)\n            {\n                // Add one to the deferred dequeue count for the value being enqueued now.\n                snappedDeferredDequeueCount = _deferredDequeueCount + 1;\n                _deferredDequeueCount = 0;\n            }\n            else\n            {\n                // Ensure that the increment of _deferredDequeueCount below won't overflow.\n                sim_assert(_deferredDequeueCount < (1 << bitsizeof(count_t<Size>)) - 1);\n\n                snappedDeferredDequeueCount = 0;\n                _deferredDequeueCount++;\n            }\n        }\n\n        // Allow 0 or more threads to to get a value out\n        _readSemaphore.post_multiple(snappedDeferredDequeueCount);\n\n        return snappedDeferredDequeueCount;\n    }"}
{"prompt": "Return the larger of x and y.", "completion": "template<auto Width, auto Nan>\ninline uint<Width> max_internal(uint<Width> x, uint<Width> y)\n{\n    return minmax<Width, Nan>(gt_internal<Width>, x, y);\n}"}
{"prompt": "Function: switch_MT_sets  Switches the MT sets.  Current becomes next and vice versa.  The caller must ensure it is safe to call this function.  Specifically: * no other thread is calling switch_MT_sets concurrently and * no thread is reading/writing the contents of either the current or next MT array while this function is running.  The next() function relies on the _retwist_semaphore to ensure that its call to switch_MT_sets is safe.", "completion": "inline void switch_MT_sets()\n{\n// Invert these 1-bit integers\ncurrent_mt = ~current_mt;\nnext_mt = ~next_mt;\n}"}
{"prompt": "Read a 32-bit word from hart's data memory.", "completion": "inline int_t dmem_read\n ( hart_index_t hid  //< Hart index.\n , uint_t addr       //< DMEM byte address to read from.\n ) {\n    return core.dmem_read(hid, checked_cast<dmem_addr_t>(addr));}"}
{"prompt": "// Function: done_reduce_fn// Reduce function for finding done schedule entries. Each iteration// returns the number of leading done entries and whether all entries// captured by the reduce node are done.", "completion": "inline done_t done_reduce_fn(done_t x, done_t y){\n    done_t output;\n    if (x.full)\n    {\n        // Add the number of done entries from y to x\n        output.count = x.count + y.count;\n    }\n    else {\n        // x is not full so cannot take entries from y\n        output.count = x.count; }\n    // Output is only full if both x and y are full\n    output.full = x.full && y.full;\n    return output;}"}
{"prompt": "Increment-only counter that will saturate rather than overflow.", "completion": "template<auto Width>\nclass saturating_counter\n{\npublic:\n    using count_t=uint<Width>;\n\nprivate:\n    count_t _count = 0;\n\npublic:"}
{"prompt": "Floating-point subtraction.", "completion": "inline float32 sub(float32 x, float32 y)\n{\n    return F32::float32_sub_denormoff(x, y);\n}"}
{"prompt": "Enqueue zero or one values into each FIFO.", "completion": "void enqueue_many\n ( optional<T>[ArraySize] values //< Array of values to store.  `is_valid` indicates if value should be stored.\n ) {\n  static for (const auto i : ArraySize)  {\n    if (values[i].is_valid) {\n       _fifos[i].enqueue(values[i].value);\n   }\n}}"}
{"prompt": "Puts a value into the cache.", "completion": "void put\n( Key key               //< The key to write.\n, set_index_t set_index //< The hashed value of the key.\n, Value value           //< The new value to put.\n)\n{\nget_or_put(false, key, set_index, value);\n}"}
{"prompt": "Return the first element in the vector. Equivalent to `v.data[0]`. If the vector is empty then a `sim_assert` is triggered.", "completion": "template <typename T, auto N /*< Maximum size.*/>\nstruct vector\n{\n    T[N] data;\n    count_t<N> size;\n}\n\ntemplate <typename T, auto N>\ninline T front(vector<T, N> v)\n{\n    sim_assert(v.size > 0);\n    return at(v, 0);\n}\n"}
{"prompt": "Implements map-reduce. Inputs are first mapped into the appropriate result type, and then reduced to a single output using a binary reduction tree.", "completion": "template <typename T, auto N, typename R> inline R map_reduce((T) -> R map_fn, (R, R) -> R reduce_fn, T[N] x) { return reduce(reduce_fn, map(map_fn, x)); }"}
{"prompt": "Flip function parameters", "completion": "inline auto flip((auto, auto) -> auto f) { return [f](auto x, auto y) { return f(y, x); }; }"}
{"prompt": "Set the size of FFT you would like to process, which must be a power of 2. // To assist the hardware, also provide `log2(FFT size)` and `log2(FFT size / 2 / NumProcs) - 1`. This function should be called once for a given FFT size. When using multiple threads, all threads need to use the same parameters. `fft_size_in` must be a power of 2, between `NumProcs * 16` and `MaxFFTSize`, inclusive.", "completion": "void set_parameters(fft_size_t fft_size_in, fft_size_log2_addr_t fft_size_log2_in, num_iterations_per_processor_log2_addr_t num_iterations_per_processor_log2_minus1_in)\n    {\n        _fft_size = fft_size_in;\n        _fft_size_log2 = fft_size_log2_in;\n        _num_iterations_per_processor = fft_size_in >> 1 >> _num_proc_log2;\n        _num_iterations_per_processor_log2_minus1 = num_iterations_per_processor_log2_minus1_in;\n        _num_threads_launched = _fft_size_log2 * _num_iterations_per_processor;\n        _expected_input_transfers = (_fft_size / InputWordWidth) - 1;\n        _expected_output_transfers = (_fft_size / OutputWordWidth) - 1;\n\n        num_iterations_per_processor_log2_addr_t twiddle_factor_reuse_log2 = _fft_size_log2 - 1 - _num_proc_log2;\n        twiddle_addr_t twiddle_base_address = 0;\n\n        //The twiddle base address must accumulate over the stages, so it is computed via a loop here\n        for(const auto i : _fft_size_log2)\n        {\n            auto stage_num = _fft_size_log2 - i - 1;\n            \n            _twiddle_base_address_array[stage_num] = twiddle_base_address;\n\n            if(static(IoType == FftInputOutputType::NaturalInputReverseOutput))\n            {\n                twiddle_base_address += _num_iterations_per_processor >> ((stage_num <= _crossover_stage) ? 0 : stage_num - _crossover_stage);\n            }\n            else\n            {\n                fft_size_log2_addr_t level = _fft_size_log2 - 1 - stage_num;\n                twiddle_base_address += (level <= _crossover_stage) ? 1 : 1 << (level - _crossover_stage);\n            }\n        }\n        //Initialize the _completed_thread variable\n        for(const auto i : NumThreads)\n        {\n            _completed_thread[i] = -2;\n        }\n    }"}
{"prompt": "A reference to a previously-produced sequence of output bytes.", "completion": "template<auto WindowSize, auto MaxLength>\nstruct reference\n{\n    count_t<WindowSize> offset;\n    count_t<MaxLength> length;\n}"}
{"prompt": "Atomically interleave updates among multiple threads to internal state of type `T[N]`. Return both old and new element value. A state is maintained per each call site of this inline function.", "completion": "template\n< auto N              //< Size of state array. Must be between 2 and 8 inclusive.\n, typename T          //< Type of the internal state.\n, T InitialValue = {} //< Initial value.>inline pair<T, T> atomically_one_of(index_t<N> i, (T) -> T modify)\n{\n    static assert(N >= 2 && N <= 8);\n\n    pair<T, T> result;\n\n    [[schedule(N)]]\n    {\n        // TODO: fix when generic initializer syntax is available\n        static if (N == 2)\n        {\n            static T[N] state = {InitialValue, InitialValue};\n        }\n        else static if (N == 3)\n        {\n            static T[N] state = {InitialValue, InitialValue, InitialValue};\n        }\n        else static if (N == 4)\n        {\n            static T[N] state = {InitialValue, InitialValue, InitialValue, InitialValue};\n        }\n        else static if (N == 5)\n        {\n            static T[N] state = {InitialValue, InitialValue, InitialValue, InitialValue, InitialValue};\n        }\n        else static if (N == 6)\n        {\n            static T[N] state = {InitialValue, InitialValue, InitialValue, InitialValue, InitialValue, InitialValue};\n        }\n        else static if (N == 7)\n        {\n            static T[N] state = {InitialValue, InitialValue, InitialValue, InitialValue, InitialValue, InitialValue, InitialValue};\n        }\n        else static if (N == 8)\n        {\n            static T[N] state = {InitialValue, InitialValue, InitialValue, InitialValue, InitialValue, InitialValue, InitialValue, InitialValue};\n        }\n\n        result = {state[i], modify(state[i])};\n        state[i] = result.second;\n    }\n\n    return result;\n}"}
{"prompt": "Floating-point multiplication.", "completion": "inline float32 mul(float32 x, float32 y)\n{\n    return F32::float32_mul_denormoff(x, y);\n}"}
{"prompt": "Execute a task asynchronously. A call to `async_exec` is implemented as a dataflow fork. For the common case of passing a lambda to `async_exec`, the lambda entry FIFO holds the value of each variable captured by the lambda. If the lambda pipeline does not introduce backpressure, the compiler can optimize away the entry FIFO.", "completion": "inline void async_exec(() -> void task)\n{\n    async_then_void(task, [](){});\n}"}
{"prompt": "Fail test with the specified tag when the condition is false. Test failure triggers `sim_assert`, prints information about failing test and the supplied message, and sets global status to the test tag.", "completion": "inline void assert_string(tag_t tag, bool cond, const string message)\n{\n    if (!cond)\n    {\n        println(message);\n        print_failure(tag);\n        sim_assert(false);\n    }\n}"}
{"prompt": "Return ReadWidthBits of data starting at the given argument. In the sequential read case, this would read the newest word into the cache and shift the cached values. This function is capable of tracking what the last read address is, and will not update the cache if a new word does not need to be read. In the non-sequential case, it is the responsibility of the caller to refill the cache with valid data.", "completion": "read_word_t read(input_bit_index_t offset){\n    auto addr = offset / (MemoryWidthBytes * BitsPerByte);\n    index_t<MemoryWidthBits> shift = offset % MemoryWidthBits;\n    optional<index_t<MaxInputFileSizeWords>> snapped_last_read_addr;\n    atomic {\n       snapped_last_read_addr = _last_read_addr;\n        _last_read_addr = {true, addr};\n    }\n    bool read = (!snapped_last_read_addr.is_valid || addr != snapped_last_read_addr.value);\n    auto max_addr = (_input_size + MemoryWidthBytes - 1) / MemoryWidthBytes - 1;\n    memory_word_t data;\n    if (read) {\n        data = _input_buffer.read(template min<index_t<MaxInputFileSizeWords>>(addr + (CacheWords - 1), max_addr));\n    }\n    memory_word_t[CacheWords] snapped_cache;\n    atomic {\n        static memory_word_t[CacheWords] _cache;\n        if (read) {\n            static for(const auto i : CacheWords) {\n                memory_word_t rhs;\n                if (i + 1 < CacheWords)\n                    rhs = _cache[i + 1];\n                else\n                    rhs = data;\n                _cache[i] = rhs;\n            }\n        }\n        snapped_cache = _cache;\n    }\n    return cast<uint<CacheWords * MemoryWidthBits>>(snapped_cache) >> shift;\n}"}
{"prompt": "Subtract amount from the counter.", "completion": "inline void subtract(ctr_t amount) {\n    atomic\n    {\n        _second = modular::add(_second, amount);\n    }\n}"}
{"prompt": "Combine two vectors into a vector of pairs. Any remaining elements of the longer vector are dropped. The maximum size of the zipped vector is the minimum of `N` and `M`.", "completion": "\nimport .options as opt\nimport data.array as A\nimport data.function\nimport data.optional as Opt\nimport data.order as Ord\nimport data.tuple as P\nimport sync.atomic\nimport numeric.int.operator\nimport numeric.int.operator.unsigned as unsigned\nimport type.stdtype\nimport data.vector as V\n\ntemplate <typename T, auto N /*< Maximum size.*/>\nstruct vector\n{\n    T[N] data;\n    count_t<N> size;\n}\n\ntemplate <typename S, auto N, typename T, auto M>\ninline auto zip(vector<S, N> s, vector<T, M> t)\n{\n    return zip_with(P::make_pair<S, T>, s, t);\n}\n"}
{"prompt": "Initialize stack pointers. Not necessary if the firmware startup code performs the initialization. When the option `Option::HartsShareDMEM` is set then each hart is allocated its own stack of the specified size, with the stack for hart 0 starting at the specified address, and stacks for remaining harts following below it in DMEM address space.", "completion": "inline void init_stack_pointers\n     ( uint_t stack_start //< Start address of the stack. Must be 16 byte aligned.\n                          // Since the stack grows down, the starting address usually\n                           // is set to `DMEM_ORIGIN + DMEM_LENGTH - 0x10`\n      , uint_t stack_size  //< Size of the stack for each hart.\n       ) {\n     sim_assert(running == false);\n      static for(const auto hid : HARTS) {\n          // By ABI calling convention stack pointer should be 16 bytes aligned.\n          sim_assert((stack_start & 0xf) == 0);\n          // Don't generate invalid stack pointers\n          sim_assert(stack_start - stack_size >= DMEM_ORIGIN && stack_start < DMEM_ORIGIN + DMEM_LENGTH ||\n                     stack_start - stack_size >= MMIO_ORIGIN && stack_start < MMIO_ORIGIN + MMIO_LENGTH);\n           core.register_set(hid, ABI::sp, cast<int_t>(stack_start));\n           static if ((CONFIG & Option::HartsShareDMEM) == Option::HartsShareDMEM)\n           {\n               // Set a different stack pointer for each hart.\n               stack_start -= stack_size;\n           }\n       }\n    }"}
{"prompt": "Return true if there is no more data in the queue.", "completion": "inline bool empty()\n    {\n        return _lateCounter.count() == 0;\n    }"}
{"prompt": "A class faciliating the async/await pattern. `async` spawns a new asynchronous thread in which the provided lambda will be executed. Upon completion, the lambda's return value will be stored into an internal FIFO. In parallel, a call to `await` will block the calling thread until a result from `async` is available. The standard thread ordering guarantee ensures that no threads can overtake another and results will be returned by `await` in the same order as they entered `async`.", "completion": "template\n    < typename T          //< Type returned by lambda argument to `async`.\n    , auto ReturnDepth    //< Depth of return value FIFO (maximum number of `fork<i>` results that can\n                          //  be buffered ahead of a corresponding `async` call). A recommended value\n                          //  would be 32 allowing it to be implemented in LUTRAM.\n    , auto WaitForDepth   //< Depth of the wait-for FIFO (maximum number of threads that can queue\n                          //  inside `await`). A conservative value would be 32 allowing it to be\n                          //  implemented in LUTRAM, however, for lambdas that typically take more\n                          //  than 32 cycles to return, this should be increased.\n    >\nclass async_await\n{\n    FIFO<T, ReturnDepth, true /* EnqueueBlocking */, false /* DequeueBlocking */> _returnFifo;\n    // Use a separate counter so that it can be atomically check-and-decremented independently\n    // of the FIFO dequeue operation.\n    // This counter cannot overflow since it is only incremented after a successful blocking\n    // _returnFifo.enqueue() call\n    counter<ReturnDepth, 0> _returnCount;\npublic:\n    //| Spawn a new thread that calls the provided lambda.\n    // Out-of-order behaviour can occur if multiple calls (causing multiple\n    // inlines) to the same method on the same instance exists and is thus\n    // not recommended.\n    inline void async(() -> T task)\n    {\n        async_then<T>(task,\n            [](T value)\n            {\n                _returnFifo.enqueue(value);\n                _returnCount.increment();\n            }\n        );\n    }\n    //| Check to see if the async function has returned.\n    // It is exposed as a public function to provide the option for a consumer\n    // to wait on multiple `async_await` instances simultaneously (e.g. `fork_join`).\n    inline bool check()\n    {\n        auto count = _returnCount.count();\n        return count != 0;\n    }\n    //| Decrement the return counter if argument is true.\n    inline void decrement(bool value)\n    {\n        sim_assert(!value || _returnCount.count() != 0);\n        _returnCount.subtract(cast<uint1>(value));\n    }\n    //| Wait for task started by `async` to complete and return its result.\n    T await()\n    {\n        inline bool check_and_decrement()\n        {\n            auto result = check();\n            decrement(result);\n            return result;\n        }\n        [[fifo_depth(WaitForDepth)]] wait_for(check_and_decrement());\n        return dequeue();\n    }\n    //| Pop the value returned by the lambda function to `async`.\n    // This function is called by `await`.\n    // It is exposed as a public function since `check`, which would typically\n    // be called from a `wait_for`, is unable to return anything but the check\n    // result. The intention is to allow the consumer to `wait_for(check_and_decrement())`\n    // and then `dequeue`.\n    inline T dequeue()\n    {\n        return _returnFifo.dequeue();\n    }\n};"}
{"prompt": "Increment the semaphore count by the specified `amount`, potentially waking one or more threads.", "completion": "void post_multiple(sem_ctr_t amount)\n    {\n        sim_assert((cast<sem_ctr_t>(_counter.count()) + amount) <= M);\n        _counter.add(amount);\n    }"}
{"prompt": "Throughput: one multiplication per clock cycle. Execution Time: numIter*4 clock cycles", "completion": "uint32 mul(uint32 x) {\n    return x * 3;}void ExecuteOneReplica(uint32 numIter) {\n    pipelined_for(numIter, [](uint32 tid) {\n        static multiply_by_three _multiplier;\n        uint32[4] src;\n        uint32[4] dst;\n        static for (const auto i : 4) {\n            dst[i] = _multiplier.mul(src[i]);\n        }\n    });\n}"}
{"prompt": "Process up to `MaxBytes` of new input data.", "completion": "inline uint<Width> compute\n        ( count_t<ThreadCount> identifier   //< Which of the `ThreadCount` CRCs to update.\n        , uint8[MaxBytes] input             //< Input data.\n        , count_t<MaxBytes> input_bytes     //< Number of valid bytes in the input data.\n        , bool reset                        //< Indicates if CRC value should be reset before processing `input`.  Should be set to `true` on the first call associated with a particular input stream.\n        )\n    {\n        const auto table = generate();\n\n        // Because the CRC calculator operates on a reversed polynomial\n        // input bits are reversed in order to produce the same output\n        // as a normal (not reversed) polynomial.\n        // If ReflectInput is true, then input bits are not reversed\n        // which produces the same output as a CRC operating on a normal polynomial\n        // and reversed inputs.\n        if(!ReflectInput)\n        {\n            static for(const auto i : MaxBytes)\n            {\n                input[i] = reverse<uint8>(input[i]);\n            }\n        }\n\n        // The initial value is given in big-endian.\n        const auto initial = reverse<uint<Width>>(Init);\n        const auto input_bits = input_bytes * 8;\n        const auto off_bits = max_bits - input_bits;\n\n        // crc(r1, d) = crc(r1, 0) + crc(0, d). crc(0, d) depends on the width of input\n        const auto off_input = cast<uint<max_bits>>(cast<uint<max_bits>>(input) << off_bits);\n        const uint<Width> reduced_crc = crc_lookup(table, off_input);\n\n        uint<Width> crc_local;\n        atomic\n        {\n            // crc(r1, 0) depends on the width of remainer\n            uint<Width> crc_keep = reset ? initial : _crc_results[identifier];\n            uint<Width> updated_crc = crc_map(table, crc_keep, checked_cast<count_t<data_segments>>(off_bits / 4));\n\n            // combine crc(0,d) and crc(r,0)\n            crc_local = updated_crc ^ reduced_crc ^ (crc_keep >> input_bits);\n            _crc_results[identifier] = crc_local;\n        }\n\n        // Similar to the handling of ReflectInput, only reverse output bits\n        // when ReflectOutput is false (because the CRC operates on a reversed polynomial)\n        return (ReflectOutput ? crc_local : reverse<uint<Width>>(crc_local)) ^ XorOut;\n    }"}
{"prompt": "Negate input.", "completion": "inline bid32_t neg(bid32_t x)\n{\n    auto binary = cast<decimal32PackedFormat_t>(x);\n    binary.sign = ~binary.sign;\n    return cast<bid32_t>(binary);\n}"}
{"prompt": "Write a word to a word aligned address", "completion": "inline void write_aligned(addr_t addr, word_t word)\n    {\n        mem.write_aligned(addr, cast<uint8[BytesPerWord]>(word));\n    }"}
{"prompt": "S-type instruction format", "completion": "struct S_type\n{\n    Opcode  opcode;\n    uint5   imm_4_0;\n    Funct3  funct3;\n    Reg     rs1;]n    Reg     rs2;\n    uint7   imm_11_5;\n}inline int32 S_immediate(S_type s)\n{\n    return sign_extend(concat(s.imm_11_5, s.imm_4_0));\n}"}
{"prompt": "Write one word. Maybe block until there is space in the buffer.", "completion": "void write(T[WordSize] word)\n    {\n        // Wait for a free entry\n        inline bool entry_available()\n        {\n            bool result = (_freeCounter.count() != 0);\n            if (result)\n            {\n                _freeCounter.subtract(1);\n            }\n            return result;\n        }\n        wait_for(entry_available());\n\n        // Increment write pointer\n        addr_t localWriteAddr;\n        atomic\n        {\n            localWriteAddr = _writeAddr;\n            _writeAddr++;\n        }\n        // Write to memory\n        _cyclicBufferMem.write_aligned((localWriteAddr % Depth) * WordSize, word);\n    }"}
{"prompt": "Return the square root of x. This function provides better accuracy than calling `invsqrt` and then multiplying by x.", "completion": "template<auto I, auto F>\ninline optional<fixed<I, F>> sqrt(fixed<I, F> x)\n{\n    const auto value = x.value;\n    optional<fixed<I, F>> result;\n    result.is_valid = value >= 0;\n\n    const auto leadingOneIndex = highest_one<uint<I + F>>(value);\n    int6 exponent = leadingOneIndex.value - F;\n    const auto significandFbit = I + F - 1;\n    const uint<I + F> significand = value << (I + F - 1 - leadingOneIndex.value);\n\n    // compute 1/sqrt(x)\n    const auto invsqrtValueFbit = I + F;\n    const uint<1 + invsqrtValueFbit> invsqrtValue = invsqrt_internal<significandFbit, invsqrtValueFbit>(exponent, significand);\n\n    // sqrt(x) = value * 1/sqrt(x)\n    const auto sqrtFbit = invsqrtValueFbit + F;\n    const auto sqrt = value * invsqrtValue;\n\n    const auto shiftedValueFbit = F + 1;\n    const auto halfExponent = (exponent >> 1) + (exponent & 1);\n    sim_assert(sqrtFbit - shiftedValueFbit + halfExponent >= 0);\n    const auto shiftedValue = sqrt >> (sqrtFbit - shiftedValueFbit + halfExponent);\n\n    result.value.value = (shiftedValue >> 1) + (shiftedValue & 1);\n    return result;\n}"}
{"prompt": "B-type instruction format", "completion": "struct B_type\n{\n    Opcode  opcode;\n    uint1   imm_11;\n    uint4   imm_4_1;\n    Funct3  funct3;\n    Reg     rs1;\n    Reg     rs2;\n    uint6   imm_10_5;\n    uint1   imm_12;\n}\ninline int32 B_immediate(B_type b)\n{\n    const uint1 zero_0 = 0;\n    return sign_extend(concat(b.imm_12, b.imm_11, b.imm_10_5, b.imm_4_1, zero_0));\n}"}
{"prompt": "Returns maximum value in the array of integers.", "completion": "template <typename T, auto N> inline T maximum(T[N] x) { return reduce(selecting(Ord::greater_than), x); }"}
{"prompt": "In the following example the accumulate function computes the sum of a series of integers and returns the current sum. Each call to accumulate adds one more integer to the running total. accumulate uses an static local variable to hold the current sum. This variable is reset when the is_last parameter is true. If accumulate is called from 2 call sites in parallel, then it will not operate as desired. The problem is that calls from the 2 call sites will interleave and thus the running total will not have the expected value.", "completion": "uint32 accumulate(uint32 value, bool is_last) {\n    uint32 result;\n    atomic {\n        static uint32 _sum = 0;\n        result = _sum + value;\n        _sum = is_last ? 0 : result;\n    }\n    return result;\n}\n// assume function1 and function2 run in parallel\nvoid function1() {\n    pipelined_for(1000, [](uint32 i) {\n        bool is_last = (i == 999);\n        uint32 result = accumulate(i, is_last);\n        if (is_last) {\n            println('result from function1: ', result);\n        }\n    });}\nvoid function2() {\n    pipelined_for(500, [](uint32 i) {\n        bool is_last = (i == 499);\n        uint32 result = accumulate(i, is_last);\n        if (is_last) {\n            println('result from function2: ', result);\n        }\n    });\n}"}
{"prompt": "Perform an inclusive scan on the given vector.", "completion": "template <typename T, auto N /*< Maximum size.*/>\nstruct vector\n{\n    T[N] data;\n    count_t<N> size;\n}\n\ntemplate <typename R, typename T, auto N>\ninline vector<R, N> inclusive_scan(vector<T, N> input, (R, R) -> R AssociativeFn)\n{\n    R[2][N] results;\n    static for (const auto i : N)\n    {\n        results[0][i] = cast<R>(input.data[i]);\n    }\n    static for (const auto j : clog2(N))\n    {\n        const uint1 prev_index = j % 2;\n        const uint1 next_index = (j + 1) % 2;\n        static for (const auto i : N)\n            if (i < input.size)\n            {\n                const auto shifted_by_j = 1 << j;\n                if (i < shifted_by_j)\n                    results[next_index][i] = results[prev_index][i];\n                else\n                {\n                    const auto idx = i - shifted_by_j;\n                    results[next_index][i] = AssociativeFn(results[prev_index][i], results[prev_index][idx]);\n                }\n            }\n    }\n    return {results[clog2(N) % 2], input.size};\n}"}
{"prompt": "Read one column from FIFO and discard `discard_column_count` subsequent columns from the same square.", "completion": "array_t dequeue_and_discard(index_t<ArraySize> discard_column_count)\n    {\n        count_t<ArraySize> total_column_count = discard_column_count + 1;\n\n        // Block the calling thread until the containing square is available\n        _read_semaphore.wait_multiple(total_column_count);\n\n        // Get the value of the read pointer, and increment the read pointer\n        decomposed_addr consumer_index = increment_address(total_column_count);\n\n        // Read one element from each memory\n        array_t result;\n\n        static for (const auto i : ArraySize)\n        {\n           result[i] = _data_memories[i][consumer_index.array_index];\n        }\n\n        // The memory addressing and muxing that occurs during a write gets the data\n        // in almost the desired form.  Each element of the output array is in 1 memory\n        // but output element 'i' might be in memory 'j'.  A rotation addresses this.\n        result = rotate_array_right<ArraySize>(result, consumer_index.index_within_square);\n\n        auto array_end = consumer_index.index_within_square + total_column_count;\n\n        // discard_count can only be used to discard values within the same square\n        sim_assert(array_end <= ArraySize);\n\n        if (array_end == ArraySize)\n        {\n            // An entire square has been dequeued\n            _write_semaphore.post_multiple(ArraySize);\n        }\n\n        return result;\n    }"}
{"prompt": "Returns a function with the signature: `(optional<A>, optional<B>)->optional<decltype(fn(A, B))>`. The result is valid if and only if both inputs are valid. `fn` is called to compute the resulting value. If the result is invalid, then `fn` is not called, and the result value is undefined.", "completion": "inline auto lift2_optional((auto, auto) -> auto fn) {\n    return [fn](auto x, auto y)\n    {\n        optional<decltype(fn(x, y))> result = {};\n        if (x.is_valid && y.is_valid)\n        {\n            result = make_optional(true, fn(x.value, y.value));\n        }\n        return result;\n    };\n}\n"}
{"prompt": "Decrement the semaphore count by 1 if it is nonzero otherwise block.", "completion": "void wait()\n    {\n        wait_multiple(1);\n    }"}
{"prompt": "Take in one or more ciphertexts and return the decrypted plaintexts. If `store_key` is true, then store and use the passed key. If `use_new_key` is true (but not `store_key`), then use the passed key but do not store it. If `store_key` and `use_new_key` are both false, use the stored key.", "completion": "template<typename T>\n    T decrypt(T ciphertext, bool use_new_key, bool store_key, key_t new_key)\n    {\n        static assert(bitsizeof(T) >= bitsizeof(text_t));\n        static assert(bitsizeof(T) % bitsizeof(text_t) == 0);\n        const auto N = bitsizeof(T) / bitsizeof(text_t);\n\n        key_t snapped_key = new_key;\n        atomic\n        {\n            static key_t key;\n            if (store_key)\n            {\n                key = new_key;\n            }\n            if (!use_new_key && !store_key)\n            {\n                // Use stored key\n                snapped_key = key;\n            }\n        }\n\n        // Generate new round keys\n        auto expanded_keys = _expand_keys(snapped_key);\n        // Perform decryption\n        text_t[N] ciphertext_array = cast<text_t[N]>(ciphertext);\n        text_t[N] plaintext;\n        static for (const auto i : N)\n        {\n            plaintext[i] = _decrypt_with_expanded_keys(ciphertext_array[i], expanded_keys);\n        }\n        return cast<T>(plaintext);\n    }"}
{"prompt": "// Create 512 threads and test mutexes", "completion": "// Create 512 threads\npipelined_for(512, [](index_t<512> id) {\n    static mutex _mutex;\n    _mutex.lock();\n    // This counter is used for testing, verifies that \n    // only 1 thread is between lock/unlock\n    // The counter is initialized to 0\n    static counter<512, 0> _counter;\n    // Increment the counter to 1\n    _counter.increment();\n    // Extend the pipeline depth to enable many threads\n    // in between lock/unlock if there is a bug in the mutex\n    stages<10>();\n    // There should only be 1 thread in between lock/unlock\n    if (1 != _counter.count()) {\n        atomic {\n            _errorCount++;\n        }\n    }\n    _counter.decrement();\n    _mutex.unlock();\n});"}
{"prompt": "Return the nearest integer not greater in magnitude than `x`.", "completion": "template<Denorm denorm_mode>\ninline uint64 trunc(uint64 x)\n{\n    return trunc_internal<uint64, _mantissa_width, _exponent_width, _exponent_bias, denorm_mode>(x);\n}"}
{"prompt": "Combines two arrays into an array of pairs.", "completion": "template <typename T1, typename T2, auto N> inline P::pair<T1, T2>[N] zip(T1[N] x, T2[N] y) { return zip_with(P::make_pair<T1, T2>, x, y); }"}
{"prompt": "Subtract `in2` from `in1`.", "completion": "template <Denorm denorm_mode>\ninline uint64 sub(uint64 in1, uint64 in2)\n{\n    return add<denorm_mode>(in1, neg(in2));\n}"}
{"prompt": "Ceiling.", "completion": "template<auto I, auto F>\ninline optional<fixed<I, F>> ceil(fixed<I, F> x)\n{\n    auto value = x.value;\n    optional<fixed<I, F>> result;\n    int<I> integ = value >> F;\n    uint<F> frac = value;\n\n    result.value.value = cast<decltype(result.value.value)>((frac == 0 ? integ : integ + 1) << F); // error checking below\n    result.is_valid = (value < 0) || (result.value.value >= 0);\n    return result;\n}"}
{"prompt": "Return the sum and carry generated by adding  `a`, `b`, and `carry`. Expected iteration order while calling this over multiple words: Least significant to most significant", "completion": "template\n    < typename T //< Type of values to be added\n    >inline pair<T, uint1> add_with_carry\n    ( T a //< The first operand for addition\n    , T b //< The second operand for addition\n    , uint1 carry //< The input carry\n    )\n{ \n    // The sum with carry can be computed by expression: a + b + carry\n    // but that will generate two T bit adders. The expression below \n    // uses only one T+1 bit adder. The LSB of left operand of '+' \n    // below is 1 and LSB of the second operand is carry. Due to their\n    // addition, the incoming carry for LSB of a and b is same as the \n    // carry passed as paramter. The final right shift removes the \n    // sum of 1 and carry from the result.\n    auto sum_w_carry = (((a << 1) | 1) + ((b << 1) | carry)) >> 1;\n    auto carry_out = cast<uint1>(sum_w_carry >> bitsizeof(T));\n    auto sum = cast<T>(sum_w_carry);\n    return make_pair(sum, carry_out);\n}"}
{"prompt": "Map array of input values to result values.", "completion": "template <typename T, auto N> inline auto map((T) -> auto f, T[N] x) { using result_t = decltype(f(x[0]));  result_t[N] r;  static for(const auto i : N) { r[i] = f(x[i]); }     return r; }"}
{"prompt": "Helper function that blocks the current thread until the output is not full", "completion": "void enqueue_blocking(T value)\n    {\n        // Wait for an output slot to be available\n        wait_for(check_and_reserve());\n\n        // Write the data\n        enqueue(value);\n    }"}
{"prompt": "Transform an array into a pair of arrays using a projection.", "completion": "template\n    < typename L //< Type of first projection of T.\n    , typename R //< Type of second projection of T.\n    , typename T //< Input array element type.\n    , auto N     //< Array length.\n    >\ninline P::pair<L[N], R[N]> unzip_with\n    ( (T) -> P::tuple2<L, R> f  //< Mapping from input array element to pair of\n                             //  result elements.\n    , T[N] xs\n    )\n{\n    P::pair<L[N], R[N]> result;\n    static for(const auto i : N)\n    {\n        auto p = f(xs[i]);\n        result.first[i] = p.first;\n        result.second[i] = p.second;\n    }\n    return result;\n}"}
{"prompt": "Enqueue a flit to channel `input_index`. The caller must also specify some metadata about the flit.", "completion": "void enqueue\n      ( uint32 packet_number //< An ID for the packet used for reordering.\n                              // These must be sequential starting at 0.\n       , bool end_of_packet   //< Last flit of the packet.\n       ) {\n       // Track number of flits written for this packet\n        uint32 flits_written;\n        atomic {\n           _current_flits_written++;\n           flits_written = _current_flits_written;\n            // Reset for next packet\n            if (end_of_packet) {\n                _current_flits_written = 0; } }\n        // Create metadata entry\n        packet_reorderer_metadata_entry_t entry;\n        entry.packet_number = packet_number;\n        entry.flits_written = flits_written;\n        entry.write_done = end_of_packet;\n        entry.valid = true;\n        // Write to metadata memory\n        _metadata_mem[_write_addr] = entry;\n        // If end of packet, advance write pointer\n        if (end_of_packet)\n        {\n            _write_addr = modular::increment(_write_addr);\n            // Block overwriting metadata that has not been read\n            _write_semaphore.wait();\n        }\n    }"}
{"prompt": "Bind a value to the function argument", "completion": "inline auto bind((auto) -> auto f, auto x) { return [f, x]() { return f(x); }; }"}
{"prompt": "Moving this function back into writer_t causes \"Internal compiler error\" since it is calling a method inside a fellow nested class (codeLengthCount) inline pair<uint<gz::MaxLengthCodeBits + 7>, count_t<gz::MaxLengthCodeBits + 7>>", "completion": "writer_dot_write_code_lengths(index_t<HLIT> tid){\n    sim_assert(codeLengthCount.journal_count() != 0);\n    auto entry = codeLengthCount.journal_get(tid);\n    sim_assert(entry.sym <= 18);\n    auto sym = entry.sym;\n    auto c = writer.len_code(sym);\n    uint<gz::MaxLengthCodeBits + 7> data = c.code;\n    count_t<gz::MaxLengthCodeBits + 7> size = c.length;\n    sim_assert(size > 0);\n    data |= entry.repeatFor << size;\n    if (sym == 16) {\n        sim_assert(entry.repeatFor < 4);\n        size += 2;\n    }\n    else if (sym == 17) {\n        sim_assert(entry.repeatFor < 8);\n        size += 3;\n    }\n    else if (sym == 18) {\n        size += 7;\n    }\n    else{\n        sim_assert(entry.repeatFor == 0);\n    }\n    return {data, size};\n}"}
{"prompt": "Speculatively apply a state update to each possible current state and return all results.", "completion": "template\n    < auto StateCount               //< Total number of possible states.\n    , typename SpeculativeUpdate    //< The output type of `fn`.\n    >\ninline auto speculate_updates\n    ( (index_t<StateCount>) -> SpeculativeUpdate fn  //< Function which is called `StateCount` times with values equal to either:\n                                                     // `0, 1, 2, ..., StateCount-1`\n                                                     // or\n                                                     // `override.value, override.value, ..., override.value`\n    , optional<index_t<StateCount>> override         //< If valid, then all returned results are equal to `fn(override.value)`.\n                                                     // Useful for cases where downstream code should behave as if it ignores the \"current\" state.\n    )\n{\n    return map_indices([override, fn](index_t<StateCount> i)\n        {\n            return fn(from_optional(i, override));\n        });\n}"}
{"prompt": "Speculatively precompute how the vector of inputs will affect NFA state, given all possible current NFA states. `updates[i]` describes how to update the NFA state if the current NFA state has state `i` enabled.", "completion": "\nstate_t[StateCount] updates = speculate_updates([input, table_elements](index_t<StateCount> vertex_id)\n        {\n            // Determine how the inputs will affect state\n            // if the NFA state before processing those inputs\n            // has `vertex_id` enabled.\n           state_t curr_state = binary_to_one_hot<StateCount>(vertex_id);\n           static for (const auto input_element_index : InputWidth)\n           {\n               if (input_element_index < input.size) {\n                state_t updated_state = {};\n                table_element_t table_element = table_elements[input_element_index];\n                static for (const auto src_vertex : StateCount)\n                {\n                    if (curr_state[src_vertex]) \n                    { \n                        out_edges_t out_edges = table_element[src_vertex];\n                        static for (const auto dst_vertex : StateCount)\n                        {\n                            // Reduce resource consumption by only checking \n                            // updates which are deemed possible by the MaxBackwardJump and MaxForwardJump parameters \n                            if (static((dst_vertex == src_vertex) || \n                                       ((dst_vertex < src_vertex) && ((src_vertex - dst_vertex) <= (MaxBackwardJump * (input_element_index + 1)))) || \n                                       ((dst_vertex > src_vertex) && ((dst_vertex - src_vertex) <= (MaxForwardJump * (input_element_index + 1)))))) \n                            { \n                                static for (const auto out_edge_index : MaxOutDegree) \n                                { \n                                    if (out_edge_index < out_edges.size) \n                                    { \n                                        vertex_t out_vertex = out_edges.data[out_edge_index];\n                                        if (out_vertex == dst_vertex) {\n                                            updated_state[dst_vertex] = true;  \n                                        } } } } } } }  \n                curr_state = updated_state; } }\n        return curr_state;    },\n    {});"}
{"prompt": "Implement a binary reducer tree using a function to reduce a pair of inputs.", "completion": "template <typename T, auto N>\ninline T reduce((T, T) -> T f, T[N] x)\n{\n    static if (N == 1)\n    {\n        return x[0];\n    }\n    else\n    {\n        // Combine adjacent elements in x\n        const auto NewN = (N + 1) / 2;\n        T[NewN] new_array;\n        static for(const auto i : N/2)\n        {\n            new_array[i] = f(x[2*i], x[2*i + 1]);\n        }\n        // carry odd case\n        static if ((N % 2) == 1)\n        {\n            new_array[NewN - 1] = x[N - 1];\n        }\n        return reduce(f, new_array);\n    }\n}"}
{"prompt": "Rotate the elements of an arbitrary-typed array left by the given number", "completion": "inline T[M] rotate_array_left(T[N] data, U amount) { sim_assert(amount < N);  return rotate_array<M, T, N, U, false>(data, amount); }"}
{"prompt": "Return sign of `x`.", "completion": "inline bool signbit(bfloat16 x)\n{\n    const auto s = cast<bfloat16PackedFormat>(x);\n    return s.sign == 1;\n}"}
{"prompt": "Print dynamic instruction trace. Can be used as `trace` handler.", "completion": "inline void print_trace(auto hid, auto pc, auto instr, Decoded decoded, optional<int32> value)\n{\n    atomic\n    {\n        print(hid, \"0x\");\n        print_hex(cast<uint32>(pc << 2));\n        print(\":\\t\");\n        print_hex(instr);\n        print(\"\\t\");\n        print_disasm(cast<uint32>(pc << 2), instr, decoded);\n        if (value.is_valid)\n        {\n            if (decoded.format.rd.is_valid)\n            {\n                print(\"\\t\");\n                print_reg(decoded.format.rd.value);\n                print(\" <-\", value.value);\n            }\n            else if (decoded.store)\n            {\n                print(\"\\tmem <-\", value.value);\n            }\n        }\n        print(\"\\n\");\n    }\n}"}
{"prompt": "Reset cyclic buffer. This is not thread-safe and should not be called while any other functions are running.", "completion": "void reset()\n    {\n        _cyclicBuffer.reset();\n    }"}
{"prompt": "Return array `bool[N]` with element at i the result of i <= arg.", "completion": "import data.array\nimport data.optional\nimport type.coerce\nimport type.stdtype\n\ntemplate <auto N, typename T>\ninline bool[N] mask_greater_than(T arg)\n{\n    sim_assert(arg <= N);\n    return reinterpret_cast<bool[N]>(static((1 << N) - 2) << arg);\n}\n\ntemplate <auto N, typename T>\ninline bool[N] mask_less_equal(T arg)\n{\n    sim_assert(arg <= N);\n    auto ret = mask_greater_than<N>(arg);\n    static for(const auto i : N)\n        ret[i] = !ret[i];\n    return ret;\n}\n\n\n\n\n"}
{"prompt": "Return a count of the number of bits set in the value.", "completion": "import data.array\nimport data.optional\nimport type.coerce\nimport type.stdtype\n\ntemplate <typename T> \ninline bitcount_t<T> pop_count(T value) { \n\t// Reduction tree to improve area of synthesized structure\n    const auto Width = bitsizeof(T); \n    bitcount_t<T>[Width] cts;  \n    uint1[Width] value_as_bits = cast<uint1[Width]>(value);  // Populate cts array from initial bit vector\n    static for (const auto i : Width) { \n    cts[i] = value_as_bits[i]; }\n    return sum<bitcount_t<T>>(cts);}\n"}
{"prompt": "Returns a function with the signature: `(optional<A>)->optional<decltype(fn(A))>`. The result is valid if and only if the input is valid.// `fn` is called to compute the resulting value. // If the result is invalid, then `fn` is not called, and the result value is undefined.", "completion": "\ninline auto lift_optional((auto) -> auto fn)\n{\n    return [fn](auto x)\n    {\n        optional<decltype(fn(x))> result = {};\n        if (x.is_valid)\n        {\n            result = make_optional(true, fn(x.value));\n        }\n\n        return result;\n    };\n}\n"}
{"prompt": "Floating-point multiply-accumulate.", "completion": "inline float32 mad(float32 x, float32 y, float32 z)\n{\n    return fmad32(x, y, z);\n}"}
{"prompt": "Unwrap a value of a newtype", "completion": "template <typename T>\ninline T unwrap_newtype(newtype<T> x)\n{\n    return x.unwrap;\n}"}
{"prompt": "Return the nearest integer not greater in magnitude than `x`.", "completion": "template<Denorm denorm_mode>\ninline bfloat16 trunc(bfloat16 x)\n{\n    return trunc_internal<bfloat16, _mantissa_width, _exponent_width, _exponent_bias, denorm_mode>(x);\n}"}
{"prompt": "Apply exponent bias and add mantissa implied one.", "completion": "template<Denorm denorm_mode>\ninline float32ExpandedFormat unpackFloat32(float32PackedFormat a)\n{\n    float32ExpandedFormat a_PostUnpack;\n    a_PostUnpack.sign = a.sign;\n\n    if (denorm_mode == Denorm::Off)\n    {\n        a_PostUnpack.exponent = cast<int9>(a.exponent) - 127;\n        a_PostUnpack.mantissa = a.mantissa | 1 << 23;\n    }\n    else\n    {\n        a_PostUnpack.exponent = (a.exponent == 0) ? -126 : cast<int9>(a.exponent) - 127;\n        a_PostUnpack.mantissa = (a.exponent == 0) ? a.mantissa : a.mantissa | (1 << 23);\n    }\n\n    return a_PostUnpack;\n}"}
{"prompt": "Wait for the entry at `addr` to be written and then read the value. The caller should ensure that this is not called for a free entry.", "completion": "T read(addr_t addr /*< Element address.*/)\n    {\n        auto value = _cyclicBuffer.read(addr);\n        return value[0];\n    }"}
{"prompt": "Map array of indices to result values.", "completion": "template <auto N> inline auto map_indices((index_t<N>) -> auto f) { return map(f, template indices<N>()); }"}
{"prompt": "Spawn `count` threads executing `body`, which can be a lambda or a function taking one argument specifying the thread index from the range [0, `count`]. The function returns an array `T[N]` of values produced by `body`. `N` must not be greater than `count`.", "completion": "template <auto N, typename I, typename T>\ninline T[N] pipelined_map(auto count, (I) -> T body)\n{\n    sim_assert(count <= N);\n    sim_assert(count <= (1 << bitsizeof I));\n\n    const auto fn = [body](I x) -> T\n    {\n        return body(x);\n    };\n\n    static pipelined<I, decltype(fn)> loop;\n\n    return loop.go(count, fn);\n}"}
